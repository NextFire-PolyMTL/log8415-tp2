\documentclass[letterpaper,headings=standardclasses,parskip=half]{scrartcl}

% \usepackage[french]{babel}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}

% \usepackage[backend=bibtex]{biblatex}

% \addbibresource{references.bib}

\newcommand{\todo}{{\color{red}{TODO}}}

\titlehead{
    \centering
    \includegraphics[width=2in]{Signature_electronique 150.jpg}
}
\subject{LOG8415E - Advanced Concepts of Cloud Computing}
\title{Lab 2 – Deploying Webapps on Containers on AWS}
\subtitle{Report}
\author{
    Quentin Guidée (2206809) 
    \and
    Aurel Lucrich Ikama Honey (2160742)
    \and
    Nam Vu (2230468)
}
\date{\today}
\publishers{Polytechnique Montréal – Fall 2023}

\lstdefinestyle{lststyle}{
    basicstyle=\ttfamily
}
\lstset{style=lststyle}

\begin{document}
\maketitle
\thispagestyle{empty}

\clearpage
\pagenumbering{arabic}
\tableofcontents

\clearpage
\section*{Handover Documentation Report}

\subsection*{Project Title}

Cluster Benchmarking using EC2 Virtual Machines and Elastic Load Balancer (ELB)

\subsection*{Report Date}

\today

\subsection*{Team Members}

\begin{itemize}
    \item Quentin Guidée (2206809)
    \item Aurel Lucrich Ikama Honey (2160742)
    \item Nam Vu (2230468)
\end{itemize}

\noindent\rule{\textwidth}{0.3pt}

\section*{Executive Summary}

To continue learning about Cloud Computing, we have been tasked to create a cluster of EC2 instances on AWS to run inference ML models.
In the continuation of the previous project, everything is automated using Python and the Boto3 SDK.
The infrastructure of this second project consists of a custom orchestrator, which dispatches requests to multiple ML models running on EC2 instances. Each EC2 instance runs two ML models.
This report presents the work done, the results obtained and the lessons learned.

\noindent\rule{\textwidth}{0.3pt}

\section{Introduction}

The goal of this second project is to continue experimenting with AWS.
This time, the load balancer will be abstracted by writing a custom orchestrator.
This orchestrator will be responsible for dispatching requests to multiple workers.
Finally, the workers will be used to run inference on machine learning models.

\section{Project objectives}

Different objectives are fixed for this project:
\begin{itemize}
    \item Create and deploy a custom orchestrator on Amazon EC2.
    \item Deploy multiple workers, running by pairs on Amazon EC2.
\end{itemize}

\section{Approach}

First of all, the code from the previous project has been reused, except for the load balancer and the flask app.
This allowed us to quickly have a working solution.

Then, the load balancer has been replaced by a custom orchestrator. This consists of a simple flask app, which dispatches requests to the workers and manages instances state.

Finally, the workers have been deployed. Each EC2 instance runs two workers, which are both listening on different ports.

Like the previous project, all the code is versioned in a Git monorepo. The Python project is managed using the Poetry dependency manager: this makes it easier to manage dependencies across the different subprojects and automate the venv setup.

\section{Tasks and Responsoibilities}

\subsection*{Nam Vu}

\begin{itemize}
    \item Adapt the code from the previous project to the new requirements.
    \item Docker manifests.
\end{itemize}

\subsection*{Quentin Guidée}

\begin{itemize}
    \item Create the orchestrator.
\end{itemize}

\subsection*{Aurel Lucrich Ikama Honey}

\begin{itemize}
    \item Create the workers.
    \item Benchmarking script.
\end{itemize}

\section{Progress and achievements}

Initially, we took over some of the code from the previous project. We reused the deployment code for the EC2 instances and the Flask application.

As in the previous project, we've created a bootstrap script to build our infrastructure. This script is used to create the EC2 instances in which we've deployed two Docker containers with Docker Compose. In each of these containers, we have deployed a Flask application.

This Flask application as indicated in the subject calls a function that is responsible for performing inference on an ML model.

\subsection{Workers}

In this project we used the machine learning Transformer model called
DistilBERT. This is a reduced but faster version of the BERT model that enables
bidirectional text prediction from a given unlabeled text. EC2 instances are
deployed in a security group that allows SSH and HTTP connections from the
outside. The instances are called workers and there are 4 of them. In short, we
have 4 workers, each containing two docker containers (with a machine learning application). See Figure \ref{fig:infra}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/infra.pdf}
    \caption{Infrastructure diagram}
    \label{fig:infra}
\end{figure}

Each worker is setup using a docker-compose file which creates two containers in each worker (see \texttt{.docker/worker.docker-compose.yml})

\subsection{Orchestrator}

We then created an orchestrator to dispatch requests to all workers. This orchestrator is a Flask application that manages the state of EC2 instances and dispatches requests to all workers.

When a request is received, the orchestrator first tries to find a worker that is not busy. If all workers are busy, the request is added to a queue. When a worker becomes available, the first request in the queue is dispatched to this worker.

Note that a mutex is used to ensure that requests are handled in a thread-safe manner.

\subsection{Tests}

Finally, to test our infrastructure, we used a test script in the bench module, which executes a given number (here 5) of requests in parallel on the orchestrator. We used the python library \emph{multiprocessing} to create a pool of processes to execute the requests in parallel.

\section{Results and Outcomes}

\subsection{Workers}

First, for the workers, we can see in the logs below that the docker-compose file is working as expected, creating two containers. Two sample requests are also been sent manually to the workers to test them.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/worker.png}
    \caption{Worker logs}
    \label{fig:worker}
\end{figure}

\subsection{Orchestrator}

Then, for the orchestrator, we can see in the logs below that the requests are being dispatched to the workers as expected. The orchestrator is also able to handle the case where all workers are busy. This case can be seen in the logs with the mention of a "Queue length".

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/orchestrator.png}
    \caption{Orchestrator logs}
    \label{fig:orchestrator}
\end{figure}

\subsection{Tests}

Finally, when running our test script, we can see that the requests are being dispatched and received by the workers as expected.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/test.png}
    \caption{Test script logs}
    \label{fig:test}
\end{figure}

\section{Lessons learned}

\begin{itemize}
    \item The code from project 1 was very modular, which made it easy to adapt it to the new requirements. So good code design is important to support future changes.
    \item While the orchestrator of this project is very simple compared to real world solutions, this project demonstrates that for some use cases, a custom orchestrator can be a good and lightweight solution.
    \item Workers allows us to scale our application horizontally. This is a good way to handle a large number of requests which require a lot of computation but can be processed independently.
    \item Containers make it easier to deploy applications requiring a lot of dependencies (such as ML ones). This is a good way to ensure that the application will run on any machine and will not be affected by the host environment.
\end{itemize}

\section{Recommendations}

\begin{itemize}
    \item The orchestrator could also be scaled horizontally to handle more requests and reduce the risk of Single Point of Failure.
    \item The Docker images could be build once and pushed to a registry, instead of being built on each worker. This would reduce the deployment time of new workers. A CI/CD pipeline could be used to automate this process.
\end{itemize}

\section{Conclusion}

In this work, we were able to achieve the objectives set: create and deploy a custom orchestrator on Amazon EC2 and deploy multiple workers. Each worker is setup with the help of a docker-compose file. Two containers are deployed in each worker, each containing ML models.

Limit cases have also been managed, such as the case where all workers are busy. In this case, the orchestrator will wait for a worker to be available before dispatching the request, using a queue.

\addsec{Attachments}

\subsection*{Git repository}

GitHub: \url{https://github.com/NextFire-PolyMTL/log8415-tp2}

List of authors:

\begin{itemize}
    \item Quentin Guidée:
          \begin{lstlisting}
Quentin Guidée <quentin.guidee@gmail.com>
\end{lstlisting}
    \item Aurel Lucrich Ikama Honey:
          \begin{lstlisting}
aurpur <122250820+aurelikama@users.noreply.github.com>
\end{lstlisting}
    \item Nam Vu:
          \begin{lstlisting}
NextFire <git@yuru.moe>
\end{lstlisting}
\end{itemize}

\addsec{Signatures}

Quentin G., Aurel L.I.H., Nam V.

% \printbibliography

\end{document}
